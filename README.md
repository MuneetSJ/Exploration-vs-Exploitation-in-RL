# Exploration-vs-Exploitation-in-RL
A project that aims to conduct a quantitative assessment of the impact of the exploration-exploitation trade-off on state-of-the-art reinforcement learning algorithms on several different benchmark task in environments from the OpenAi gym. 

#Abstract
A significant problem in reinforcement learning is the replicability of the results, with many researchers unable to reproduce the work conducted by others. OpenAI created their gym with numerous environments to try and create benchmark environments to be used by Reinforcement learning agents. Another problem is finding the balance between exploration and exploitation by an agent in any environment, which can significantly impact performance.
This report aims to implement different exploration strategies (Epsilon-greedy, Boltzmann, Upper Confidence Bound, Gaussian noise and Orensien-Uhlenbeck noise) state-of-the-art algorithms, which include Deep-Q network, SARSA, DDPG and NAF/CDQN in OpenAI gym. This would allow a like-by-like comparison between the different models and strategies to highlight optimal and best approaches when implementing reinforcement learning algorithms.
How well an agent performs in an environment is recorded and quantified with a range of metrics stored in a Weights and biases(WandB). This allows for a numerical comparison between the different agents and the environments to be made, storing important metrics like episode rewards, loss and mean Q value of the agent.
The second task is to ensure that the findings and results are easily accessible, where any machine learning practitioner can find the work and replicate the findings. To achieve this goal, OpenAI gym is used for the environment, and easy to implement Agents from the library Keras-rl are implemented to improve the replicability of the work presented. The project will also be uploaded to GitHub as a public repository where WandB reports containing all the numerical data can also be found.

#How to run
1.Select the agent you would like to run
2.Using either cp(cartpole),mc(Mountain car),ll(Lunar Lander) or pd(Pendulum) change the import used by the agetns
3.Run agent

N.B the runs log the metrics and data to WandB remove the callback to just run or log in with an account to log metrics

#View all findings
Sarsa
https://wandb.ai/msjaswal/computing-project/reports/All-SARSA-runs--Vmlldzo2ODA3MTE

Naf
https://wandb.ai/msjaswal/computing-project/reports/ALL-NAF-runs--Vmlldzo2ODA3ODA

DDPG
https://wandb.ai/msjaswal/computing-project/reports/All-DDPG-runs--Vmlldzo2ODA3NDE

DQN
https://wandb.ai/msjaswal/computing-project/reports/All-DQN-runs-and-performances--Vmlldzo2ODA2MzQ

Using these links are preffered over the PDF reports
