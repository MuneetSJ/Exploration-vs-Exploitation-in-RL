from tensorflow.keras.optimizers import Adam
from rl.agents import NAFAgent
from rl.memory import SequentialMemory
from rl.callbacks import WandbLogger
from exploration_policies import *
import numpy as np
import mountaincar.model_continuous as mc
import pendulum.model as pd

# builds the agent used to train from the actions and model created for the environment

def build_agent(v_model, l_model, mu_model, actions):
    memory = SequentialMemory(limit=100000, window_length=1)
    policy = OrnsteinUhlenbeckPolicy(theta=.65, size=mc.actions, sigma=.3)
    naf_a = NAFAgent(nb_actions=actions, V_model=v_model, L_model=l_model, mu_model=mu_model, memory=memory,
                     nb_steps_warmup=100, random_process=policy, gamma=.99, target_model_update=1e-3)
    return naf_a


naf = build_agent(mc.build_actor(), mc.build_critic(), mc.build_mu_model(), mc.actions)


naf.compile(Adam(lr=1e-2, clipnorm=1), metrics=['mse'])

naf.fit(mc.env, nb_steps=50000, visualize=False, verbose=1,
        callbacks=[WandbLogger(project='Computing-Project', entity='msjaswal')])

naf.save_weights('latest_model/{}/naf_weights.h5f'.format('MountainCar'), overwrite=True)

scores = naf.test(mc.env, nb_episodes=15, visualize=True)
print(np.mean(scores.history['episode_reward']))
