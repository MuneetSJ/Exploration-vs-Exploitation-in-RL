
from tensorflow.keras.optimizers import Adam
from rl.agents import DDPGAgent
from rl.memory import SequentialMemory
from rl.callbacks import WandbLogger
from exploration_policies import *
import numpy as np
import mountaincar.model_continuous as mc
import pendulum.model as pd

# builds the agent used to train from the actions and model created for the environment

def build_agent(actor, critic, critic_action_input):
    memory = SequentialMemory(limit=100000, window_length=1)
    policy = OrnsteinUhlenbeckPolicy(theta=.15, sigma=.3, size=pd.actions)
    ddpg_a = DDPGAgent(nb_actions=pd.actions, actor=actor, critic=critic, critic_action_input=critic_action_input,
                       memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100, random_process=None,
                       gamma=.99, target_model_update=1e-3)
    return ddpg_a


ddpg = build_agent(pd.build_actor(), pd.build_critic(), pd.action_input)
ddpg.compile(Adam(lr=.0001, clipnorm=1), metrics=['mse'])


ddpg.fit(pd.env, nb_steps=50000, visualize=False, verbose=1,
         callbacks=[WandbLogger(project='Computing-Project', entity='msjaswal')])

ddpg.save_weights('latest_model/{}/ddpg_weights.h5f'.format('LunarLander'), overwrite=True)

scores = ddpg.test(pd.env, nb_episodes=15, visualize=True)
print(np.mean(scores.history['episode_reward']))
