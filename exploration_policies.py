from rl.util import *
from rl.policy import Policy
from rl.random import AnnealedGaussianProcess


class RandomPolicy(Policy):
    def select_action(self, q_values):
        assert q_values.ndim == 1
        actions = q_values.shape[0]

        action = np.random.choice(range(actions))
        return action


class EpsGreedyPolicy(Policy):
    def __init__(self, eps=.1):
        super().__init__()
        self.eps = eps

    def select_action(self, q_values):
        assert q_values.ndim == 1
        actions = q_values.shape[0]

        if np.random.uniform() < self.eps:
            action = np.random.randint(0, actions)
        else:
            action = np.argmax(q_values)
        return action


class BoltzmannPolicy(Policy):
    def __init__(self, tau=1.0, clip=(-500., 500.)):
        super(BoltzmannPolicy, self).__init__()
        self.tau = tau
        self.clip = clip

    def select_action(self, q_values):
        assert q_values.ndim == 1
        q_values = q_values.astype('float64')
        actions = q_values.shape[0]

        exp_values = np.exp(np.clip(q_values / self.tau, self.clip[0], self.clip[1]))
        probs = exp_values / np.sum(exp_values)
        action = np.random.choice(range(actions), p=probs)
        return action


class UpperConfidenceBound(Policy):
    def __init__(self, confidence=1):
        super(UpperConfidenceBound, self).__init__()
        self.confidence = confidence

    def select_action(self, q_values):
        assert q_values.ndim == 1
        q_values = q_values.astype('float64')

        if self.agent.step == 0:
            self.action_counts = np.ones(q_values.shape)

        uncertainty = self.confidence * (np.sqrt(np.log(self.agent.step)/self.action_counts))
        upper_bounds = q_values + uncertainty

        action = np.argmax(upper_bounds)
        self.action_counts[action] += 1
        return action


class GaussianWhiteNoisePolicy(AnnealedGaussianProcess):
    def __init__(self, mu=0., sigma=1., sigma_min=None, n_steps_annealing=1000, size=1):
        super(GaussianWhiteNoisePolicy, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min,
                                                       n_steps_annealing=n_steps_annealing)
        self.size = size

    def sample(self):
        sample = np.random.normal(self.mu, self.current_sigma, self.size)
        self.n_steps += 1
        return sample


class OrnsteinUhlenbeckPolicy(AnnealedGaussianProcess):
    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, size=1, sigma_min=None, n_steps_annealing=1000):
        super(OrnsteinUhlenbeckPolicy, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min,
                                                      n_steps_annealing=n_steps_annealing)
        self.theta = theta
        self.mu = mu
        self.dt = dt
        self.size = size
        self.reset_states()

    def sample(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(
            self.dt) * np.random.normal(size=self.size)
        self.x_prev = x
        self.n_steps += 1
        return x

    def reset_states(self):
        self.x_prev = np.random.normal(self.mu, self.current_sigma, self.size)

